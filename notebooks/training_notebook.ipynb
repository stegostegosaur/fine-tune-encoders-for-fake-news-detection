{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning a Transformer for Fake News Detection\n",
    "\n",
    "This notebook contains the complete pipeline for fine-tuning a transformer model (like BERT or DistilBERT) for text classification, based on the `train.py` script. The process includes:\n",
    "\n",
    "1.  **Configuration**: Setting up all experiment parameters.\n",
    "2.  **Helper Functions**: Defining logic for layer freezing and metrics computation.\n",
    "3.  **Data Loading & Preparation**: Loading the processed dataset and creating a validation split.\n",
    "4.  **Tokenization**: Preparing the text data for the model.\n",
    "5.  **Model Setup**: Loading the model and applying the layer freezing strategy.\n",
    "6.  **Training**: Running the fine-tuning process using the Hugging Face `Trainer`.\n",
    "7.  **Saving & Pushing to Hub**: Saving the final model and optionally pushing it to the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akossteger/.pyenv/versions/3.10.6/envs/FakeBERT/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "import accelerate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from huggingface_hub import HfFolder, notebook_login\n",
    "from datasets import Dataset, DatasetDict\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['do_eval', 'eval_strategy', 'per_device_eval_batch_size', 'per_gpu_eval_batch_size', 'eval_accumulation_steps', 'eval_delay', 'jit_mode_eval', 'bf16_full_eval', 'fp16_full_eval', 'eval_steps', 'eval_do_concat_batches', 'batch_eval_metrics', 'eval_on_start', 'eval_use_gather_object']\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "import inspect\n",
    "sig = inspect.signature(TrainingArguments.__init__)\n",
    "print([param for param in sig.parameters.keys() if 'eval' in param.lower()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Freezing Logic\n",
    "\n",
    "This function applies the chosen freezing strategy ('all', 'half', or 'none') to the model's encoder layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_layers(model, model_type, freeze_mode, num_total_layers_bert=12, num_total_layers_distilbert=6):\n",
    "    logger.info(f\"Applying freeze mode: {freeze_mode} for model type: {model_type}\")\n",
    "    if freeze_mode == \"none\": # Full fine-tuning\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "        logger.info(\"All layers are trainable (full fine-tuning).\")\n",
    "        return\n",
    "\n",
    "    if model_type == 'bert':\n",
    "        encoder_layers = model.bert.encoder.layer\n",
    "        embeddings = model.bert.embeddings\n",
    "        num_total_layers = num_total_layers_bert\n",
    "    elif model_type == 'distilbert':\n",
    "        encoder_layers = model.distilbert.transformer.layer\n",
    "        embeddings = model.distilbert.embeddings\n",
    "        num_total_layers = num_total_layers_distilbert\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "\n",
    "    # Freeze embeddings by default when freezing encoder layers\n",
    "    logger.info(\"Freezing embedding layers.\")\n",
    "    for param in embeddings.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    if freeze_mode == \"all\":\n",
    "        num_layers_to_freeze = num_total_layers\n",
    "    elif freeze_mode == \"half\":\n",
    "        num_layers_to_freeze = num_total_layers // 2\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported freeze mode: {freeze_mode}. Choose 'all', 'half', or 'none'.\")\n",
    "\n",
    "    logger.info(f\"Total encoder layers: {num_total_layers}. Layers to freeze: {num_layers_to_freeze}.\")\n",
    "\n",
    "    for i, layer in enumerate(encoder_layers):\n",
    "        if i < num_layers_to_freeze:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            for param in layer.parameters(): # Ensure subsequent layers are trainable\n",
    "                param.requires_grad = True\n",
    "\n",
    "    # Ensure the classifier head is always trainable\n",
    "    trainable_classifier = False\n",
    "    if hasattr(model, 'classifier') and model.classifier is not None:\n",
    "        for param in model.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "        trainable_classifier = True\n",
    "    if hasattr(model, 'pre_classifier') and model.pre_classifier is not None: # For DistilBERT\n",
    "         for param in model.pre_classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "         trainable_classifier = True\n",
    "\n",
    "    if trainable_classifier:\n",
    "        logger.info(\"Classifier head parameters are set to trainable.\")\n",
    "    else:\n",
    "        logger.warning(\"Could not find a standard classifier head to ensure it's trainable.\")\n",
    "\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    logger.info(f\"Number of trainable parameters: {trainable_params} / {total_params} ({100 * trainable_params / total_params:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics Computation\n",
    "\n",
    "This function calculates accuracy, F1-score, precision, and recall. It will be passed to the `Trainer` to evaluate the model on the validation set during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    precision = precision_score(labels, preds, average='weighted')\n",
    "    recall = recall_score(labels, preds, average='weighted')\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "Instead of command-line arguments, we define all parameters in this configuration class. **This is the main place to change settings for different experiments.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingConfig:\n",
    "    # --- Model arguments ---\n",
    "    # Change for different experiments: 'distilbert-base-uncased' or 'bert-base-uncased'\n",
    "    model_name_or_path = \"distilbert-base-uncased\"\n",
    "    # Must match model_name_or_path: 'distilbert' or 'bert'\n",
    "    model_type = \"distilbert\"\n",
    "    # Change for different experiments: 'all', 'half', or 'none'\n",
    "    freeze_mode = \"half\"\n",
    "\n",
    "    # --- Data arguments ---\n",
    "    # Note: Paths are relative to the `notebooks` directory\n",
    "    train_file = \"../data/processed/train.csv\"\n",
    "    eval_file = None # Set to \"../data/processed/test.csv\" or other if you have a separate eval file\n",
    "    text_column = \"text\"\n",
    "    label_column = \"label\"\n",
    "    validation_split_size = 0.1 # Used if eval_file is None\n",
    "\n",
    "    # --- Training arguments ---\n",
    "    # Note: Paths are relative to the `notebooks` directory\n",
    "    output_dir = f\"../results/{model_type}_{freeze_mode}_notebook\"\n",
    "    num_train_epochs = 3\n",
    "    per_device_train_batch_size = 8\n",
    "    per_device_eval_batch_size = 16\n",
    "    learning_rate = 5e-5\n",
    "    weight_decay = 0.01\n",
    "    warmup_steps = 0\n",
    "    logging_steps = 100\n",
    "    save_steps = 500 # Used if save_strategy is 'steps'\n",
    "    eval_strategy = \"epoch\"\n",
    "    save_strategy = \"epoch\"\n",
    "    load_best_model_at_end = True\n",
    "    metric_for_best_model = \"f1\"\n",
    "    fp16 = torch.cuda.is_available() # Enable if you have a compatible GPU\n",
    "\n",
    "    # --- Hugging Face Hub arguments ---\n",
    "    push_to_hub = True # Set to True to push model to the Hub\n",
    "    # IMPORTANT: Change this to your username and a descriptive model name\n",
    "    hub_model_id = f\"stegostegosaur/{model_type}-{freeze_mode}-fakern\"\n",
    "    hub_token = None # Will use token from `notebook_login` or saved token\n",
    "\n",
    "# Instantiate the config\n",
    "args = TrainingConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hugging Face Hub Login\n",
    "\n",
    "If `push_to_hub` is set to `True` in the config, this cell will prompt you to log in to the Hugging Face Hub. You'll need to provide an access token with `write` permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 15:21:05,645 - INFO - Attempting to log into Hugging Face Hub...\n",
      "2025-06-10 15:21:05,651 - INFO - Token found, will use it for pushing to Hub.\n"
     ]
    }
   ],
   "source": [
    "if args.push_to_hub:\n",
    "    logger.info(\"Attempting to log into Hugging Face Hub...\")\n",
    "    hub_token_to_use = args.hub_token or HfFolder.get_token()\n",
    "    if hub_token_to_use:\n",
    "        logger.info(\"Token found, will use it for pushing to Hub.\")\n",
    "    else:\n",
    "        logger.info(\"Hub token not found. Running notebook_login().\")\n",
    "        notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load and Prepare Data\n",
    "\n",
    "This section loads the training data from the specified CSV file. If no evaluation file is provided, it will automatically split the training data to create a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 15:21:08,427 - INFO - Loading training data from: ../data/processed/train.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/processed/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading training data from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mtrain_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m train_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39meval_file:\n\u001b[1;32m      5\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading evaluation data from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39meval_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/FakeBERT/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/FakeBERT/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/FakeBERT/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/FakeBERT/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/FakeBERT/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/processed/train.csv'"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Loading training data from: {args.train_file}\")\n",
    "train_df = pd.read_csv(args.train_file)\n",
    "\n",
    "if args.eval_file:\n",
    "    logger.info(f\"Loading evaluation data from: {args.eval_file}\")\n",
    "    eval_df = pd.read_csv(args.eval_file)\n",
    "    raw_datasets = DatasetDict({\n",
    "        'train': Dataset.from_pandas(train_df),\n",
    "        'validation': Dataset.from_pandas(eval_df)\n",
    "    })\n",
    "else:\n",
    "    logger.info(f\"No evaluation file provided. Splitting training data for validation (split: {args.validation_split_size}).\")\n",
    "    train_pandas_df, eval_pandas_df = train_test_split(\n",
    "        train_df,\n",
    "        test_size=args.validation_split_size,\n",
    "        random_state=42,\n",
    "        stratify=train_df[args.label_column] if args.label_column in train_df.columns else None\n",
    "    )\n",
    "    raw_datasets = DatasetDict({\n",
    "        'train': Dataset.from_pandas(train_pandas_df),\n",
    "        'validation': Dataset.from_pandas(eval_pandas_df)\n",
    "    })\n",
    "\n",
    "logger.info(f\"Raw datasets loaded: {raw_datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Labels\n",
    "\n",
    "The model requires integer labels. We find all unique labels in our data and create `label2id` and `id2label` mappings, which are essential for both training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 14:58:40,350 - INFO - Found 2 unique labels: [0 1]. Label mapping: {0: 0, 1: 1}\n",
      "Map: 100%|██████████| 42242/42242 [00:02<00:00, 15067.06 examples/s]\n",
      "Map: 100%|██████████| 4694/4694 [00:00<00:00, 29985.19 examples/s]\n"
     ]
    }
   ],
   "source": [
    "unique_labels = train_df[args.label_column].unique()\n",
    "label2id = {int(label): i for i, label in enumerate(sorted(unique_labels))}\n",
    "id2label = {i: int(label) for label, i in label2id.items()}\n",
    "num_labels = len(unique_labels)\n",
    "\n",
    "logger.info(f\"Found {num_labels} unique labels: {unique_labels}. Label mapping: {label2id}\")\n",
    "\n",
    "def map_labels(example):\n",
    "    example[args.label_column] = label2id[example[args.label_column]]\n",
    "    return example\n",
    "\n",
    "raw_datasets = raw_datasets.map(map_labels, batched=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tokenization\n",
    "\n",
    "We load the tokenizer that corresponds to our chosen model and apply it to our datasets. This converts the text into a format the model can understand (token IDs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 14:58:43,366 - INFO - Loading tokenizer for: distilbert-base-uncased\n",
      "2025-06-10 14:58:43,828 - INFO - Tokenizing datasets...\n",
      "Map: 100%|██████████| 42242/42242 [00:20<00:00, 2093.68 examples/s]\n",
      "Map: 100%|██████████| 4694/4694 [00:02<00:00, 2031.02 examples/s]\n",
      "2025-06-10 14:59:06,332 - INFO - Tokenized datasets ready: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Unnamed: 0', 'title', 'labels', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 42242\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['Unnamed: 0', 'title', 'labels', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 4694\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Loading tokenizer for: {args.model_name_or_path}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[args.text_column], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "logger.info(\"Tokenizing datasets...\")\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True, remove_columns=[args.text_column] if args.text_column in raw_datasets['train'].column_names else None)\n",
    "\n",
    "# The Trainer expects the label column to be named 'labels'\n",
    "if args.label_column != 'labels':\n",
    "     tokenized_datasets = tokenized_datasets.rename_column(args.label_column, \"labels\")\n",
    "\n",
    "logger.info(f\"Tokenized datasets ready: {tokenized_datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Setup\n",
    "\n",
    "Now we load the pre-trained model. We provide `num_labels`, `id2label`, and `label2id` so it initializes with a classification head tailored to our specific task. Afterwards, we apply our chosen layer freezing strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 14:59:06,339 - INFO - Loading model: distilbert-base-uncased for 2-class classification.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-06-10 14:59:06,553 - INFO - Applying freeze mode: half for model type: distilbert\n",
      "2025-06-10 14:59:06,553 - INFO - Freezing embedding layers.\n",
      "2025-06-10 14:59:06,553 - INFO - Total encoder layers: 6. Layers to freeze: 3.\n",
      "2025-06-10 14:59:06,553 - INFO - Classifier head parameters are set to trainable.\n",
      "2025-06-10 14:59:06,554 - INFO - Number of trainable parameters: 21855746 / 66955010 (32.64%)\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Loading model: {args.model_name_or_path} for {num_labels}-class classification.\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# --- Apply Layer Freezing ---\n",
    "if args.freeze_mode != 'none':\n",
    "    freeze_layers(model, args.model_type, args.freeze_mode)\n",
    "else:\n",
    "    logger.info(\"No layer freezing applied (full fine-tuning). All model parameters are trainable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training\n",
    "\n",
    "Finally, we define the `TrainingArguments` and instantiate the `Trainer`. The `trainer.train()` call will start the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mhub_token:\n\u001b[1;32m     26\u001b[0m          training_args_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhub_token\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mhub_token\n\u001b[0;32m---> 28\u001b[0m training_arguments \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtraining_args_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# --- Instantiate Trainer ---\u001b[39;00m\n\u001b[1;32m     31\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     32\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     33\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_arguments,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics \u001b[38;5;28;01mif\u001b[39;00m tokenized_datasets\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     38\u001b[0m )\n",
      "File \u001b[0;32m<string>:131\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object, average_tokens_across_devices)\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/FakeBERT/lib/python3.10/site-packages/transformers/training_args.py:1738\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1736\u001b[0m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[1;32m   1737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[0;32m-> 1738\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[38;5;66;03m# Disable average tokens when using single device\u001b[39;00m\n\u001b[1;32m   1741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage_tokens_across_devices:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/FakeBERT/lib/python3.10/site-packages/transformers/training_args.py:2268\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2264\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[1;32m   2266\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2267\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m-> 2268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/FakeBERT/lib/python3.10/site-packages/transformers/utils/generic.py:67\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     65\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/FakeBERT/lib/python3.10/site-packages/transformers/training_args.py:2138\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   2137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m-> 2138\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   2139\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2140\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2141\u001b[0m         )\n\u001b[1;32m   2142\u001b[0m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[1;32m   2143\u001b[0m accelerator_state_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_configured_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "training_args_dict = {\n",
    "    \"output_dir\": args.output_dir,\n",
    "    \"num_train_epochs\": args.num_train_epochs,\n",
    "    \"per_device_train_batch_size\": args.per_device_train_batch_size,\n",
    "    \"per_device_eval_batch_size\": args.per_device_eval_batch_size,\n",
    "    \"learning_rate\": args.learning_rate,\n",
    "    \"weight_decay\": args.weight_decay,\n",
    "    \"warmup_steps\": args.warmup_steps,\n",
    "    \"logging_dir\": os.path.join(args.output_dir, 'logs'),\n",
    "    \"logging_steps\": args.logging_steps,\n",
    "    \"eval_strategy\": args.eval_strategy if 'validation' in tokenized_datasets else \"no\",\n",
    "    \"save_strategy\": args.save_strategy,\n",
    "    \"save_steps\": args.save_steps,\n",
    "    \"load_best_model_at_end\": args.load_best_model_at_end if 'validation' in tokenized_datasets else False,\n",
    "    \"metric_for_best_model\": args.metric_for_best_model if 'validation' in tokenized_datasets else None,\n",
    "    \"greater_is_better\": True if args.metric_for_best_model in [\"accuracy\", \"f1\"] else None,\n",
    "    \"fp16\": args.fp16,\n",
    "    \"report_to\": \"tensorboard\",\n",
    "}\n",
    "if args.push_to_hub:\n",
    "    training_args_dict[\"push_to_hub\"] = True\n",
    "    training_args_dict[\"hub_model_id\"] = args.hub_model_id\n",
    "    if args.hub_token:\n",
    "         training_args_dict[\"hub_token\"] = args.hub_token\n",
    "\n",
    "training_arguments = TrainingArguments(**training_args_dict)\n",
    "\n",
    "# --- Instantiate Trainer ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets.get(\"validation\"),\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics if tokenized_datasets.get(\"validation\") else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Starting model training...\")\n",
    "\n",
    "try:\n",
    "    train_result = trainer.train()\n",
    "    trainer.save_model()  # Saves the tokenizer too\n",
    "    logger.info(\"Training finished successfully.\")\n",
    "\n",
    "    metrics = train_result.metrics\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "\n",
    "    if args.push_to_hub:\n",
    "        logger.info(f\"Pushing model and tokenizer to Hugging Face Hub: {args.hub_model_id}\")\n",
    "        trainer.push_to_hub(commit_message=\"End of training from notebook\")\n",
    "        logger.info(\"Model pushed to Hub successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.exception(f\"An error occurred during training: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FakeBERT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
